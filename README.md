#  Analyse des esp√®ces mondiales de crocodiles

## Aper√ßu du projet

Ce projet utilise un ensemble de donn√©es sur les crocodiles du monde entier afin d'analyser leurs caract√©ristiques biologiques et g√©ographiques.  
L'objectif est de comprendre la distribution des esp√®ces, de pr√©dire certaines caract√©ristiques comme la taille adulte ou le poids, et d'identifier les facteurs les plus importants pour la conservation et la gestion des populations.  

Le dataset contient des informations sur :  
- Le type et la classification de l'esp√®ce  
- Les caract√©ristiques biologiques (taille, poids, √¢ge √† maturit√©)  
- La r√©partition g√©ographique  
- Les observations enregistr√©es dans le monde entier

Ce projet permettra de construire des mod√®les de r√©gression et de classification, d'√©valuer leur performance et de fournir des recommandations pratiques pour la conservation des crocodiles.


###  Compr√©hension du m√©tier (Business Understanding)

*Probl√©matique principale :*  
Comment pr√©dire certaines caract√©ristiques des crocodiles (taille, poids, classification, etc.) et identifier les facteurs influen√ßant leur distribution dans le monde, afin de soutenir la recherche et la conservation des esp√®ces‚ÄØ?

*Parties prenantes :*  
- Chercheurs et biologistes sp√©cialis√©s en crocodiles  
- Organisations de protection de la faune  
- Data scientists et analystes de donn√©es  

*Objectifs du projet :*  
- Construire des mod√®les supervis√©s pour pr√©dire la classification ou des caract√©ristiques biologiques des crocodiles (r√©gression ou classification).  
- Identifier les facteurs influen√ßant la taille, le poids et la r√©partition des esp√®ces.  
- Fournir des recommandations exploitables pour la recherche et la conservation.

### Data Understanding

L'objectif de cette √©tape est de comprendre la structure et le contenu du dataset avant de commencer la pr√©paration et la mod√©lisation.

L'ensemble de donn√©es crocodile_dataset.csv contient des informations sur plusieurs esp√®ces de crocodiles dans le monde. Chaque enregistrement inclut :  
- Le type et la classification de l'esp√®ce (genre, esp√®ce, famille)  
- Les caract√©ristiques biologiques (taille adulte, poids, √¢ge √† maturit√©, etc.)  
- La r√©partition g√©ographique et habitats naturels  
- Les observations et mesures enregistr√©es par les chercheurs  
- Les comportements et interactions avec l‚Äôenvironnement  
- Les √©ventuelles notes ou commentaires des chercheurs  

*Objectifs de l'exploration des donn√©es :*  
- Identifier les colonnes *num√©riques* et *cat√©gorielles*  
- D√©tecter les *valeurs manquantes ou aberrantes*  
- Comprendre la *distribution des esp√®ces, tailles et poids*  
- Fournir un *premier aper√ßu des relations entre les variables*, qui guidera la pr√©paration des donn√©es et le choix des mod√®les  

Cette √©tape permettra de pr√©parer un *jeu de donn√©es propre et exploitable* pour la mod√©lisation supervis√©e (r√©gression ou classification) et pour des analyses exploratoires comme le clustering ou la visualisation g√©ographique.


###  Data Preparation

Cette √©tape vise √† pr√©parer le dataset pour la mod√©lisation supervis√©e et non supervis√©e.  

*√âtapes r√©alis√©es :*  
1. *Nettoyage des donn√©es*  
   - Suppression des doublons pour √©viter les biais dans les mod√®les.  

2. *Gestion des valeurs manquantes*  
   - Colonnes num√©riques : remplissage avec la m√©diane.  
   - Colonnes cat√©gorielles : remplissage avec la valeur la plus fr√©quente.  

3. *Transformation des variables*  
   - Conversion des colonnes de type texte ou date en formats exploitables (ex. ann√©e de naissance, √¢ge, etc.)  
   - Cr√©ation de nouvelles features si n√©cessaire (ex. ratio poids/taille, √¢ge relatif).  

4. *Encodage des variables cat√©gorielles*  
   - Transformation des colonnes telles que Genre, Famille, Habitat en valeurs num√©riques √† l‚Äôaide de LabelEncoder ou OneHotEncoder.  

5. *Mise √† l‚Äô√©chelle des features num√©riques*  
   - Standardisation des colonnes comme Taille adulte, Poids, √Çge √† maturit√© pour les mod√®les sensibles √† l‚Äô√©chelle (KNN, r√©seaux neuronaux).  

6. *S√©paration des donn√©es*  
   - Cr√©ation des jeux X_train, X_test, y_train, y_test selon la variable cible choisie (ex. classification de l‚Äôesp√®ce ou pr√©diction du poids).  

Cette pr√©paration garantit que les mod√®les de classification, r√©gression, clustering et analyses avanc√©es puissent √™tre appliqu√©s efficacement et produire des r√©sultats fiables et interpr√©tables.

# üìå R√©sum√© du code d'importation des biblioth√®ques

Le code commence par importer toutes les biblioth√®ques n√©cessaires pour :

---

##  1. Manipuler les donn√©es
- *pandas* : pour charger, transformer et analyser les donn√©es tabulaires.  
- *numpy* : pour les calculs num√©riques et la manipulation de tableaux.

---

##  2. Visualiser les r√©sultats
- *matplotlib* : pour cr√©er des graphiques simples (courbes, barres, histogrammes).  
- *seaborn* : pour des visualisations plus esth√©tiques et avanc√©es (heatmaps, boxplots, corr√©lations).

---

##  3. Pr√©parer et entra√Æner des mod√®les de Machine Learning
- *train_test_split* : s√©parer le dataset en un ensemble d‚Äôentra√Ænement et un ensemble de test.  
- *LabelEncoder* : transformer les variables cat√©gorielles en valeurs num√©riques.  
- *StandardScaler* : normaliser les donn√©es pour rendre les variables comparables.  

- *RandomForestClassifier / Regressor* : mod√®les d‚Äôarbres de d√©cision robustes pour classification et r√©gression.  
- *KNeighborsClassifier / Regressor* : algorithme bas√© sur la proximit√© des points (k plus proches voisins).  

- *GridSearchCV / RandomizedSearchCV* : optimisation des hyperparam√®tres des mod√®les.

---

##  4. √âvaluer les performances des mod√®les
- *Classification* :  
  - accuracy_score  
  - precision_score  
  - recall_score  
  - f1_score  
  - confusion_matrix  
  - classification_report  

- *R√©gression* :  
  - mean_squared_error (MSE)  
  - r2_score (coefficient de d√©termination)

 python
# =========================
# Import des librairies
# =========================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, mean_squared_error, r2_score



#  Lecture du dataset

Le code permet de charger et d‚Äôavoir un premier aper√ßu des donn√©es.

---

##  1. Chargement du dataset
python
# =========================
data = pd.read_csv("crocodile_dataset.csv")

# Affichage des premi√®res lignes
data.head(10)



#  Nettoyage des doublons

Le but est de v√©rifier si certaines lignes du dataset sont pr√©sentes plusieurs fois et, si c‚Äôest le cas, de les supprimer afin d‚Äô√©viter les biais dans l‚Äôanalyse.

---

##  1. V√©rification du nombre de doublons
python
# Nettoyage des doublons
# =========================
print("Nombre de doublons :", data.duplicated().sum())

# Suppression des doublons
data = data.drop_duplicates()

# V√©rification
print("Nouvelle dimension du dataset :", data.shape)



#  Gestion des valeurs manquantes

L‚Äôobjectif est de traiter les valeurs manquantes (NaN) dans le dataset afin de garantir la qualit√© des analyses et d‚Äô√©viter les erreurs lors de l‚Äôentra√Ænement des mod√®les.

---

##  1. Identification des colonnes num√©riques
python
# =========================
#Gestion des valeurs manquantes
# =========================

# Colonnes num√©riques
numeric_cols = data.select_dtypes(include=['float64','int64']).columns
for col in numeric_cols:
    data[col] = data[col].fillna(data[col].median())

# Colonnes cat√©gorielles
categorical_cols = data.select_dtypes(include=['object']).columns
for col in categorical_cols:
    data[col] = data[col].fillna(data[col].mode()[0])

# V√©rification
print("Valeurs manquantes par colonne apr√®s traitement :")
print(data.isnull().sum())



#  Encodage des variables cat√©gorielles

Les algorithmes de Machine Learning ne peuvent pas directement traiter les *donn√©es textuelles* (ex. : Male, Female, Forest, River).  
Il faut donc convertir les colonnes *cat√©gorielles* en *valeurs num√©riques* √† l‚Äôaide d‚Äôun encodage.

---

##  1. Cr√©ation d‚Äôun encodeur
python
# =========================
#  Encodage des variables cat√©gorielles
# =========================

le = LabelEncoder()
for col in categorical_cols:
    data[col] = le.fit_transform(data[col])

# V√©rification
data[categorical_cols].head()


#  Normalisation des colonnes num√©riques

Les variables num√©riques du dataset (ex. : Observed Length (m), Observed Weight (kg), Age Class) n‚Äôont pas la m√™me *√©chelle*.  
- Exemple : la longueur est en *m√®tres* (valeurs proches de 1‚Äì6) alors que le poids est en *kg* (valeurs proches de 100‚Äì1000).  
- Certains algorithmes de Machine Learning (comme KNN, SVM, R√©gression Logistique) sont sensibles √† ces diff√©rences d‚Äô√©chelles.  

Pour √©viter qu‚Äôune variable domine les autres, on applique une *normalisation*.

---

##  1. Cr√©ation d‚Äôun standardiseur
python
#  Normalisation des colonnes num√©riques
# ===============================
scaler = StandardScaler()
data[numeric_cols] = scaler.fit_transform(data[numeric_cols])
print(data[numeric_cols].head())



#  Exemple : Classification pour pr√©dire le statut de conservation

L‚Äôobjectif de cette √©tape est de pr√©parer les donn√©es pour un mod√®le de *classification* qui pr√©dit le Conservation Status (par ex. : Endangered, Vulnerable, Least Concern, etc.) en fonction des caract√©ristiques biologiques et environnementales.

---

##  1. D√©finition de la cible
python
# Exemple : classification pour pr√©dire le statut de conservation
target_col = 'Conservation Status'
X = data.drop(target_col, axis=1)
y = data[target_col]

# V√©rification des dimensions
print("X shape :", X.shape)
print("y shape :", y.shape)


#  S√©paration des donn√©es : Train/Test

L‚Äôobjectif est de diviser le jeu de donn√©es en deux parties :  
- *Train (entra√Ænement)* : utilis√© pour apprendre le mod√®le.  
- *Test (√©valuation)* : utilis√© pour mesurer la performance du mod√®le sur des donn√©es jamais vues.

---

##  Code utilis√©
python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("X_train :", X_train.shape, "X_test :", X_test.shape)
print("y_train :", y_train.shape, "y_test :", y_test.shape)


#  Entra√Ænement d'un mod√®le Random Forest

L'objectif est de cr√©er un mod√®le de *classification* pour pr√©dire le statut de conservation des crocodiles √† partir des variables du dataset.

---

##  Code utilis√©
python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Pr√©dictions
y_pred = rf.predict(X_test)



#  √âvaluation du mod√®le Random Forest

Apr√®s l'entra√Ænement et les pr√©dictions, il est crucial de *mesurer la performance* du mod√®le sur les donn√©es de test.

---

##  Code utilis√©
python
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Accuracy
print("Accuracy :", accuracy_score(y_test, y_pred))

# Rapport complet
print("Classification Report :\n", classification_report(y_test, y_pred))

# Matrice de confusion
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Matrice de confusion - Random Forest")
plt.show()


<img src="image01.png" width="400" style="display: block; margin: 0 auto;">
<p style='text-align: center; font-style: italic; color: #7f8c8d;'>
</p>


#  Importance des variables - Random Forest

Apr√®s l'entra√Ænement d'un mod√®le Random Forest, il est possible d'identifier quelles variables ont le plus contribu√© aux pr√©dictions.

---

##  Code utilis√©
python
importances = rf.feature_importances_
features = X_train.columns

plt.figure(figsize=(10,6))
plt.barh(features, importances, color='mediumseagreen')
plt.title("Importance des variables - Random Forest")
plt.xlabel("Importance")
plt.ylabel("Variables")
plt.show()


<img src="image02.png" width="400" style="display: block; margin: 0 auto;">
<p style='text-align: center; font-style: italic; color: #7f8c8d;'>
</p>

#  Optimisation du mod√®le Random Forest avec RandomizedSearchCV

Pour am√©liorer les performances du mod√®le Random Forest, nous utilisons *RandomizedSearchCV* pour tester diff√©rentes combinaisons d‚Äôhyperparam√®tres.

---

##  Code utilis√©
python
from sklearn.model_selection import RandomizedSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]  
}
random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_grid,
    n_iter=20,
    cv=3,
    scoring='f1_weighted',
    n_jobs=-1,
    verbose=1,
    random_state=42
)

random_search.fit(X_train, y_train)
print("Meilleurs param√®tres :", random_search.best_params_)

best_rf = random_search.best_estimator_
y_pred_best = best_rf.predict(X_test)




#  Visualisation des distributions par statut de conservation

Nous utilisons des *boxplots* pour explorer comment les longueurs et poids observ√©s des crocodiles varient selon leur *statut de conservation*.

---

##  Code utilis√©
python
# Distribution des longueurs par statut de conservation
plt.figure(figsize=(12,5))
sns.boxplot(x=data['Conservation Status'], y=data['Observed Length (m)'])
plt.title("Distribution des longueurs par statut de conservation")
plt.show()

# Distribution des poids par statut de conservation
plt.figure(figsize=(12,5))
sns.boxplot(x=data['Conservation Status'], y=data['Observed Weight (kg)'])
plt.title("Distribution des poids par statut de conservation")
plt.show()



<img src="image003.png" width="400" style="display: block; margin: 0 auto;">
<p style='text-align: center; font-style: italic; color: #7f8c8d;'>
</p>

<img src="image03.png" width="400" style="display: block; margin: 0 auto;">
<p style='text-align: center; font-style: italic; color: #7f8c8d;'>
</p>


#  Pr√©paration des donn√©es pour la r√©gression

Nous voulons pr√©dire le *poids observ√©* (Observed Weight (kg)) des crocodiles √† partir des autres caract√©ristiques.

---

##  Code utilis√©
python
# Target pour r√©gression
target_col_reg = 'Observed Weight (kg)'
X_reg = data.drop(target_col_reg, axis=1)
y_reg = data[target_col_reg]

# Encodage des colonnes cat√©gorielles
categorical_cols = X_reg.select_dtypes(include=['object']).columns
le = LabelEncoder()
for col in categorical_cols:
    X_reg[col] = le.fit_transform(X_reg[col].astype(str))

# Standardisation des colonnes num√©riques
numeric_cols = X_reg.select_dtypes(include=['int64','float64']).columns
scaler = StandardScaler()
X_reg[numeric_cols] = scaler.fit_transform(X_reg[numeric_cols])

# S√©paration Train/Test
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

print("X_train_reg :", X_train_reg.shape, "X_test_reg :", X_test_reg.shape)



#  Mod√©lisation de la r√©gression

Nous utilisons un *Random Forest Regressor* pour pr√©dire le *poids observ√©* des crocodiles √† partir des caract√©ristiques du dataset.

---

##  Code utilis√©
python
rf_reg = RandomForestRegressor(random_state=42)
rf_reg.fit(X_train_reg, y_train_reg)

# Pr√©dictions
y_pred_reg = rf_reg.predict(X_test_reg)



#  √âvaluation du Random Forest Regressor

Nous √©valuons les performances du mod√®le de r√©gression en utilisant *RMSE* et *R¬≤*.

---

##  Code utilis√©
python
from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error

# Calcul du RMSE et R¬≤
rmse = root_mean_squared_error(y_test_reg, y_pred_reg)
r2 = r2_score(y_test_reg, y_pred_reg)

print("RMSE :", rmse)
print("R¬≤ :", r2)



#  Importance des variables - Random Forest Regressor

Cette section permet de comprendre quelles variables ont le plus d‚Äôinfluence sur la pr√©diction du *poids observ√© des crocodiles*.

---

##  Code utilis√©
python
importances_reg = rf_reg.feature_importances_
features_reg = X_train_reg.columns

plt.figure(figsize=(10,6))
sns.barplot(
    x=importances_reg,
    y=features_reg,
    orient='h',
    color="mediumslateblue"  # couleur unique pour la lisibilit√©
)
plt.title("Importance des variables - Random Forest Regressor")
plt.xlabel("Importance")
plt.ylabel("Variables")
plt.show()



<img src="image04.png" width="400" style="display: block; margin: 0 auto;">
<p style='text-align: center; font-style: italic; color: #7f8c8d;'>
</p>



#  Explicabilit√© avec SHAP - Random Forest Regressor

Cette section montre **comment chaque variable influence les pr√©dictions** du mod√®le Random Forest pour le poids observ√© des crocodiles.

---

##  Code utilis√©
python
import shap

# Cr√©ation de l'explainer
explainer = shap.TreeExplainer(rf_reg)
shap_values = explainer.shap_values(X_test_reg)

# Visualisation r√©sum√©e (bar plot)
shap.summary_plot(shap_values, X_test_reg, plot_type="bar")

# Visualisation d√©taill√©e (summary plot classique)
shap.summary_plot(shap_values, X_test_reg)



<img src="image05.png" width="400" style="display: block; margin: 0 auto;">
<p style='text-align: center; font-style: italic; color: #7f8c8d;'>
</p>

<img src="image06.png" width="400" style="display: block; margin: 0 auto;">
<p style='text-align: center; font-style: italic; color: #7f8c8d;'>
</p>

#  Visualisations exploratoires

Cette √©tape permet de **comprendre la distribution et les relations** entre les variables cl√©s du dataset crocodile.

---

## Longueur vs Poids selon le statut de conservation

python
plt.figure(figsize=(12,5))
sns.scatterplot(
    x=data['Observed Length (m)'], 
    y=data['Observed Weight (kg)'], 
    hue=data['Conservation Status']
)
plt.title("Longueur vs Poids selon le statut de conservation")
plt.show()

```


<img src="image07.png" width="400" style="display: block; margin: 0 auto;">
<p style='text-align: center; font-style: italic; color: #7f8c8d;'>
</p>
